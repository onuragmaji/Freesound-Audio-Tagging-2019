{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data/output\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import IPython\n",
    "import IPython.display\n",
    "import PIL\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the paarameters for Spectrogaram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Notebook in cometition to preprocess:https://www.kaggle.com/daisukelab/cnn-2d-basic-solution-powered-by-fast-ai\n",
    "\n",
    "class conf:\n",
    "    sampling_rate = 44100\n",
    "    duration = 2 # sec\n",
    "    hop_length = 347*duration # to make time steps 128\n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "    n_mels = 128\n",
    "    n_fft = n_mels * 20\n",
    "    padmode = 'constant'\n",
    "    samples = sampling_rate * duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Rows and columns masking function for one of Augumentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augument images by masking random columns and rows, Only applied for CV and Train Data\n",
    "\n",
    "\n",
    "def timeFreqMasking(conf,X: np.ndarray, num_mask=2, \n",
    "                 freq_masking_max_percentage=0.15, time_masking_max_percentage=0.2,debug_display=False):\n",
    "\n",
    "    X = X.copy()\n",
    "    all_frames_num, all_freqs_num = X.shape\n",
    "    time_num,freq_num = X.shape\n",
    "    for i in range(num_mask):\n",
    "        \n",
    "        #### Frequency Masking ####\n",
    "        freq_percentage = np.random.uniform(0.0, freq_masking_max_percentage)\n",
    "        \n",
    "        num_freqs_to_mask = int(freq_percentage * freq_num)\n",
    "        f0 = int(np.random.uniform(low=0.0, high=freq_num - num_freqs_to_mask))\n",
    "        #f0 = int(f0)\n",
    "        X[:, f0:f0 + num_freqs_to_mask] = 0\n",
    "        \n",
    "        #### Time Masking ####\n",
    "        time_percentage = np.random.uniform(0.0, time_masking_max_percentage)\n",
    "        \n",
    "        num_frames_to_mask = int(time_percentage * time_num)\n",
    "        t0 = int(np.random.uniform(low=0.0, high=time_num - num_frames_to_mask))\n",
    "        #t0 = int(t0)\n",
    "        X[t0:t0 + num_frames_to_mask, :] =0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(conf, pathname, trim_long_data):\n",
    "    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n",
    "    # trim silence\n",
    "    if 0 < len(y): # workaround: 0 length causes error\n",
    "        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n",
    "    # make it unified length to conf.samples\n",
    "    if len(y) > conf.samples: # long enough\n",
    "        if trim_long_data:\n",
    "            y = y[0:0+conf.samples]\n",
    "    else: # pad blank\n",
    "        padding = conf.samples - len(y)    # add padding at both ends\n",
    "        offset = padding // 2\n",
    "        y = np.pad(y, (offset, conf.samples - len(y) - offset), conf.padmode)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def audio_to_melspectrogram(conf, audio):\n",
    "    spectrogram = librosa.feature.melspectrogram(audio, \n",
    "                                                 sr=conf.sampling_rate,\n",
    "                                                 n_mels=conf.n_mels,\n",
    "                                                 hop_length=conf.hop_length,\n",
    "                                                 n_fft=conf.n_fft,\n",
    "                                                 fmin=conf.fmin,\n",
    "                                                 fmax=conf.fmax)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "def show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n",
    "    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n",
    "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
    "                            fmin=conf.fmin, fmax=conf.fmax)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n",
    "    x = read_audio(conf, pathname, trim_long_data)\n",
    "    mels = audio_to_melspectrogram(conf, x)\n",
    "    if debug_display:\n",
    "        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n",
    "        show_melspectrogram(conf, mels)\n",
    "    return mels\n",
    "\n",
    "def convert_wav_to_image(df, source):\n",
    "    X = []\n",
    "    for i, row in tqdm_notebook(df.iterrows()):\n",
    "        x = read_as_melspectrogram(conf, source/str(row.fname), trim_long_data=False)\n",
    "        x_color = mono_to_color(x)\n",
    "        X.append(x_color)\n",
    "    return X\n",
    "\n",
    "def get_default_conf():\n",
    "    return conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Directories for processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file .\\noisy_cv already exists.\n",
      "A subdirectory or file .\\noisy_train already exists.\n",
      "A subdirectory or file .\\noisy_test already exists.\n",
      "A subdirectory or file .\\noisy_cv_aug already exists.\n",
      "A subdirectory or file .\\noisy_train_aug already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir .\\noisy_cv\n",
    "!mkdir .\\noisy_train\n",
    "!mkdir .\\noisy_test\n",
    "!mkdir .\\noisy_cv_aug\n",
    "!mkdir .\\noisy_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file .\\curated_cv already exists.\n",
      "A subdirectory or file .\\curated_train already exists.\n",
      "A subdirectory or file .\\curated_test already exists.\n",
      "A subdirectory or file .\\curated_cv_aug already exists.\n",
      "A subdirectory or file .\\curated_train_aug already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir .\\curated_cv\n",
    "!mkdir .\\curated_train\n",
    "!mkdir .\\curated_test\n",
    "!mkdir .\\curated_cv_aug\n",
    "!mkdir .\\curated_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file .\\sub already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir .\\sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"../Applied AI/SC 2 Final/Freesound data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_curated = pd.read_csv(DATA_DIR+\"/train_curated.csv\")\n",
    "train_noisy = pd.read_csv(DATA_DIR+\"/train_noisy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4970, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_curated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19815, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_noisy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Number of labels for Each 80 Class for Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels_num</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>Accelerating_and_revving_and_vroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>Accordion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>Acoustic_guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>Applause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>Bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>Bass_drum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>Bass_guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>Bathtub_(filling_or_washing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>Bicycle_bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>300</td>\n",
       "      <td>Burping_and_eructation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels_num                              labels\n",
       "0         300  Accelerating_and_revving_and_vroom\n",
       "1         300                           Accordion\n",
       "2         300                     Acoustic_guitar\n",
       "3         300                            Applause\n",
       "4         300                                Bark\n",
       "5         300                           Bass_drum\n",
       "6         300                         Bass_guitar\n",
       "7         300        Bathtub_(filling_or_washing)\n",
       "8         300                        Bicycle_bell\n",
       "9         300              Burping_and_eructation"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_curated=mlb_train.fit_transform([ i.split(\",\") for i in list(train_noisy[\"labels\"])])\n",
    "pd.DataFrame({\"labels_num\":list(np.sum(labels_train_curated, axis=0)),\"labels\":list(mlb_train.classes_)}).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19810</th>\n",
       "      <td>fffc7128.wav</td>\n",
       "      <td>Accordion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19811</th>\n",
       "      <td>fffcf57b.wav</td>\n",
       "      <td>Acoustic_guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19812</th>\n",
       "      <td>fffd1871.wav</td>\n",
       "      <td>Water_tap_and_faucet,Sink_(filling_or_washing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19813</th>\n",
       "      <td>fffe9808.wav</td>\n",
       "      <td>Clapping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19814</th>\n",
       "      <td>ffff6da3.wav</td>\n",
       "      <td>Walk_and_footsteps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              fname                                          labels\n",
       "19810  fffc7128.wav                                       Accordion\n",
       "19811  fffcf57b.wav                                 Acoustic_guitar\n",
       "19812  fffd1871.wav  Water_tap_and_faucet,Sink_(filling_or_washing)\n",
       "19813  fffe9808.wav                                        Clapping\n",
       "19814  ffff6da3.wav                              Walk_and_footsteps"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_noisy.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splting the Data noisy and curated data into train, cv and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_cv_curated, X_test_curated, y_train_cv_curated, y_test_curated = train_test_split(train_curated[\"fname\"],train_curated[\"labels\"], test_size = 0.15, random_state = 42)\n",
    "X_train_curated, X_cv_curated, y_train_curated, y_cv_curated = train_test_split(X_train_cv_curated,y_train_cv_curated, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4970, 80)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb_train = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "labels_train = mlb_train.fit_transform([ i.split(\",\") for i in list(train_curated[\"labels\"])])\n",
    "\n",
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_test = mlb_train.transform([ i.split(\",\") for i in list(test_df[\"labels\"])])\n",
    "\n",
    "\n",
    "#mlb_cv = MultiLabelBinarizer()\n",
    "labels_cv = mlb_train.transform([ i.split(\",\") for i in list(cv_df[\"labels\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3379,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_curated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3379,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_curated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(845,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv_curated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(746,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_curated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Data\n",
    "\n",
    "X_train_cv_noisy, X_test_noisy, y_train_cv_noisy, y_test_noisy = train_test_split(train_noisy[\"fname\"],train_noisy[\"labels\"], test_size = 0.15, random_state = 42)\n",
    "X_train_noisy, X_cv_noisy, y_train_noisy, y_cv_noisy = train_test_split(X_train_cv_noisy,y_train_cv_noisy, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13473,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3369,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2973,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_noisy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating Spectrogram via librosa lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(x,name,data_type):\n",
    "    plt.interactive(False)\n",
    "    #clip, sample_rate = librosa.load(filename, sr=None)\n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    librosa.display.specshow(x, x_axis='time', y_axis='mel', \n",
    "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
    "                            fmin=conf.fmin, fmax=conf.fmax)\n",
    "    #show_melspectrogram(conf, x)\n",
    "    filename  = Path('./'+data_type+'/' + name + '.jpg')\n",
    "    fig.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    plt.close()    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    plt.close('all')\n",
    "    del name,fig,ax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram_aug(x,name,data_type):\n",
    "    \n",
    "    ### Aumentation ###\n",
    "    plt.interactive(False)\n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    librosa.display.specshow(x, x_axis='time', y_axis='mel', \n",
    "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
    "                            fmin=conf.fmin, fmax=conf.fmax)\n",
    "    filename  = './'+data_type+'/'+name+'_aug' +'.jpg'\n",
    "    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    plt.close()    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    plt.close('all')\n",
    "    del name,fig,ax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Noisy data to Spectrogram images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_default_conf()\n",
    "\n",
    "# Will unzip the files so that you can see them..\n",
    "import zipfile         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d805ffd8.wav',\n",
       " '9620e8ef.wav',\n",
       " '7ce5c78d.wav',\n",
       " '10b68b30.wav',\n",
       " 'd29ecb7a.wav',\n",
       " '74b7b200.wav',\n",
       " '4f694df8.wav',\n",
       " '97f96db0.wav',\n",
       " 'f3540f19.wav',\n",
       " '60a0615c.wav']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train_noisy)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory-in-python\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4260ebea.wav\n",
       "1    426eb1e0.wav\n",
       "2    428d70bb.wav\n",
       "3    4292b1c9.wav\n",
       "4    429c5071.wav\n",
       "Name: fname, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df=pd.read_csv(DATA_DIR+\"sample_submission.csv\")\n",
    "sub_df[\"fname\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3361/3361 [12:36<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(DATA_DIR+\"test.zip\",\"r\") as z:\n",
    "    listOfFileNames = z.namelist()\n",
    "    for fileName in tqdm(list(sub_df[\"fname\"])):\n",
    "             z.extract(fileName,\"sub/.wav\")\n",
    "             x=read_as_melspectrogram(conf, \"./sub/.wav/\"+fileName, trim_long_data=False,debug_display=False)\n",
    "             create_spectrogram(x,fileName.split(\".\")[0],\"sub\")\n",
    "             #!rm -rf ./sub/.wav/*\n",
    "\n",
    "            \n",
    "zipf = zipfile.ZipFile('sub.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir('./sub/', zipf)\n",
    "zipf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13473/13473 [1:56:25<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(DATA_DIR+\"train_noisy.zip\",\"r\") as z:\n",
    "    listOfFileNames = z.namelist()\n",
    "    for fileName in tqdm(list(X_train_noisy)):\n",
    "             z.extract(fileName,\"./noisy_train/.wav\")\n",
    "             x=read_as_melspectrogram(conf, \"./noisy_train/.wav/\"+fileName, trim_long_data=False,debug_display=False)\n",
    "             create_spectrogram(x,fileName.split(\".\")[0],\"noisy_train\")\n",
    "             create_spectrogram_aug(timeFreqMasking(conf,x),fileName.split(\".\")[0],\"noisy_train_aug\")\n",
    "             #!rm -rf noisy_train/.wav/*\n",
    "\n",
    "            \n",
    "#zipf = zipfile.ZipFile('noisy_train.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "#zipdir('/kaggle/working/noisy_train/', zipf)\n",
    "#zipf.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(DATA_DIR+\"train_noisy.zip\",\"r\") as z:\n",
    "    listOfFileNames = z.namelist()\n",
    "    for fileName in list(X_cv_noisy):\n",
    "             z.extract(fileName,\"./noisy_cv/.wav\")\n",
    "             x=read_as_melspectrogram(conf, \"./noisy_cv/.wav/\"+fileName, trim_long_data=False,debug_display=False)\n",
    "             create_spectrogram(x,fileName.split(\".\")[0],\"noisy_cv\")\n",
    "             create_spectrogram_aug(timeFreqMasking(conf,x),fileName.split(\".\")[0],\"noisy_cv_aug\")\n",
    "             #!rm -rf noisy_cv/.wav/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(DATA_DIR+\"train_noisy.zip\",\"r\") as z:\n",
    "    listOfFileNames = z.namelist()\n",
    "    for fileName in list(X_test_noisy):\n",
    "             z.extract(fileName,\"./noisy_test/.wav\")\n",
    "             x=read_as_melspectrogram(conf, \"./noisy_test/.wav/\"+fileName, trim_long_data=False,debug_display=False)\n",
    "             create_spectrogram(x,fileName.split(\".\")[0],\"noisy_test\")\n",
    "             #!rm -rf noisy_test/.wav/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(DATA_DIR+\"train_curated.zip\",\"r\") as z:\n",
    "    listOfFileNames = z.namelist()\n",
    "    for fileName in list(X_train_curated):\n",
    "             z.extract(fileName,\"curated_train/.wav\")\n",
    "             x=read_as_melspectrogram(conf, \"./curated_train/.wav/\"+fileName, trim_long_data=False,debug_display=False)\n",
    "             create_spectrogram(x,fileName.split(\".\")[0],\"curated_train\")\n",
    "             create_spectrogram_aug(timeFreqMasking(conf,x),fileName.split(\".\")[0],\"curated_train_aug\")\n",
    "             #!rm -rf curated_train/.wav/*\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curated CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  zipfile.ZipFile(DATA_DIR+\"train_curated.zip\",\"r\") as z:\n",
    "    listOfFileNames = z.namelist()\n",
    "    for fileName in list(X_cv_curated):\n",
    "             z.extract(fileName,\"./curated_cv/.wav\")\n",
    "             x=read_as_melspectrogram(conf, \"./curated_cv/.wav/\"+fileName, trim_long_data=False,debug_display=False)\n",
    "             create_spectrogram(x,fileName.split(\".\")[0],\"curated_cv\")\n",
    "             create_spectrogram_aug(timeFreqMasking(conf,x),fileName.split(\".\")[0],\"curated_cv_aug\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curated test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 746/746 [02:09<00:00,  7.04it/s]\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(DATA_DIR+\"train_curated.zip\",\"r\") as z:\n",
    "    listOfFileNames = z.namelist()\n",
    "    for fileName in tqdm(list(X_test_curated)):\n",
    "             z.extract(fileName,\"./curated_test/.wav\")\n",
    "             x=read_as_melspectrogram(conf, \"./curated_test/.wav/\"+fileName, trim_long_data=False,debug_display=False)\n",
    "             create_spectrogram(x,fileName.split(\".\")[0],\"curated_test\")\n",
    "             #!rm -rf curated_test/.wav/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data={\"Y_noisy_train\":y_train_noisy,\"Y_noisy_cv\":y_cv_noisy,\"Y_noisy_test\":y_test_noisy,\n",
    "       \"Y_curated_train\":y_train_curated,\"Y_curated_cv\":y_cv_curated,\"Y_noisy_curated\":y_test_curated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( Y_data, open( \"data_final_Y.pkl\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
